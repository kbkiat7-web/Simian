 __  __             _                   _    ___ 
|  \/  | ___  _ __ | | _____ _   _     / \  |_ _|
| |\/| |/ _ \| '_ \| |/ / _ \ | | |   / _ \  | | 
| |  | | (_) | | | |   <  __/ |_| |  / ___ \ | | 
|_|  |_|\___/|_| |_|_|\_\___|\__, | /_/   \_\___|
                             |___/                

# Simian - Your Local Chatbot.

A smart, offline, privacy-first AI assistant that runs locally on your computer using Ollama. No data leaves your machine.

## Features

- ê—ƒ **100% Private** - Everything runs locally using Ollama
- ðŸ–¥ **VS Code Extension** - Integrated coding assistant 
- á¯¤ **Web Interface** - Beautiful browser-based chat
- âš¡ **Multiple Models** - Support for LLaMA, CodeLlama, Mistral, and more

##  Quick Start

### One-Command Setup
```bash
node start.js
```

This interactive menu will:
- Auto-start Ollama
- Launch the web interface
- Install the VS Code extension
- Or just run Ollama in the background

### Direct Commands
```bash
node start.js --web        # Start web interface
node start.js --extension  # Install VS Code extension  
node start.js --ollama     # Just start Ollama
node start.js --help       # Show help
```

## ðŸ“¦ Installation

### Prerequisites
1. **Install Ollama** from [https://ollama.ai](https://ollama.ai)
2. **Pull a model**: `ollama pull llama2`
3. **Install Node.js** (if you don't have it)

### Setup
1. Clone or download this project
2. Run `node start.js` in the project folder
3. Choose your preferred interface!

## Usage

### VS Code Extension
- **Ask Questions**: `Ctrl+Shift+M` (Cmd+Shift+M on Mac)
- **Explain Code**: Select code â†’ `Ctrl+Shift+E` or right-click â†’ "Explain Code"
- **Toggle Modes**: Command Palette â†’ "Monkey: Toggle Local/Cloud Mode"

### Web Interface
- Open `http://localhost:3000` (auto-opens when using `node start.js --web`)
- Chat tab: Ask general questions
- Explain Code tab: Paste code for explanations
- Auto-connects to local Ollama

## Supported Models

### Recommended Models
- **llama2** - General purpose, great for conversations
- **codellama** - Specialized for coding tasks
- **deepseek-coder** - Excellent code understanding
- **mistral** - Fast and efficient
- **phi** - Lightweight option

### Install Models
```bash
ollama pull llama2          # General purpose
ollama pull codellama       # Coding specialist
ollama pull deepseek-coder  # Advanced coding
ollama pull mistral         # Fast responses
```

## ðŸ› ï¸ Configuration

### VS Code Settings
- `monkeyAI.useOllama`: Use local Ollama (default: true)
- `monkeyAI.ollamaModel`: Which Ollama model to use
- `monkeyAI.apiKey`: API key for cloud mode
- `monkeyAI.autoStart`: Auto-start Ollama when needed

### Web Interface
- Change Ollama URL/port in the web interface
- Select different models from dropdown
- Adjust temperature for creativity

## ðŸ“ Project Structure

```
monkey-ai/
â”œâ”€â”€ ðŸ“„ start.js              # â» Main launcher (run this!)
â”œâ”€â”€ ðŸ“„ ollama-manager.js     # âš¡ Auto Ollama starter
â”œâ”€â”€ ðŸ“„ launch-web.js         # ðŸŒ Web server launcher
â”œâ”€â”€ ðŸ“„ extension.js          # ðŸ’» VS Code extension
â”œâ”€â”€ ðŸ“„ package.json          # ðŸ“¦ Extension manifest
â”œâ”€â”€ ðŸ“„ index.html            # ðŸŽ¨ Web interface
â””â”€â”€ ðŸ“„ README.md             # ðŸ“„ This file
```

## Advanced Usage

### Run Web Interface Only
```bash
node launch-web.js
```

### Run Ollama Manager Only
```bash
node ollama-manager.js
```

### VS Code Extension Development
```bash
# Package extension
vsce package

# Install locally
code --install-extension *.vsix
```

### Custom Ollama Port
If Ollama runs on a different port, update the web interface or extension settings.

## ðŸ†˜ Troubleshooting

### "Ollama not found"
- Install Ollama from [https://ollama.ai](https://ollama.ai)
- Make sure it's in your PATH
- Try running `ollama --version`

### "No models available"
- Pull a model: `ollama pull llama2`
- Check available models: `ollama list`

### "Connection failed"
- Check if Ollama is running: `ollama serve`
- Verify port (default: 11434)
- Check firewall settings

### "Extension not working"
- Reload VS Code window
- Check output panel for errors
- Try toggling between local/cloud mode

### Web interface issues
- Check console for errors (F12)
- Verify Ollama is running
- Try different browser

## ðŸ”§ Development

### Adding New Models
1. Pull the model: `ollama pull model-name`
2. Add to the dropdown in `index.html`
3. Add to VS Code settings in `package.json`

### Custom Models
1. Open file monkeyzero-creator.js, this is a template for custom AI models.
2. Replace ${this.baseModel} with your chosen model. For lightweight models, I recommend tinyllama or phi. For any other ones, use gpt-oss, deepseek-r1, or mistral. 

### Customizing the Web Interface
- Edit `index.html` for UI changes
- Modify `launch-web.js` for server behavior
- Update `ollama-manager.js` for Ollama integration

### Extending VS Code Extension
- Add new commands in `package.json`
- Implement handlers in `extension.js`
- Use the `MonkeyAI` class methods

## ðŸ¤ Contributing

1. Fork the project
2. Create a feature branch
3. Make your changes
4. Test with `node start.js`
5. Submit a pull request

## ðŸ“œ License

MIT License - feel free to use and modify!

## ðŸ™ Acknowledgments

- [Ollama](https://ollama.ai) for making local AI accessible
- [LLaMA](https://ai.meta.com/llama/) and other model creators
- VS Code team for the excellent extension API

## ðŸ’¡ Tips

- **Performance**: Larger models (13B) are slower but more capable
- **Privacy**: All processing happens locally, no internet required
- **Models**: Try different models for different tasks
- **Memory**: Monitor RAM usage with larger models
- **Speed**: Use smaller models (7B) for faster responses

---

**Happy coding with Simian!**
